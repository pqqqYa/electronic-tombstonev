> 按照信息与网络安全的经验，李艳很喜欢出PPT上的题目。对题目进行简单的变换后，就出现在期末考试的试卷上面了，所以，请您多看看下面这些题目



> 这一章涉及到很多算法，有计算，所以说猜测会出ppt上的原题

# 1. 欧几里得距离-判断噪声点

![](附件/Pasted%20image%2020250615220901.png)

**给定条件：**

- 三维样本空间S，包含6个点
- 距离阈值 d ≥ 4
- 邻居点数阈值 p ≥ 3
- 要找出"噪声数据"

**样本点：**

- S₁ = (1,2,0)
- S₂ = (3,1,4)
- S₃ = (2,1,5)
- S₄ = (0,1,6)
- S₅ = (2,4,3)
- S₆ = (4,4,2)

### 核心概念

**噪声点判定规则：** 一个点被认为是**噪声点**，当且仅当在*距离d*范围内的*邻居点数量小于p个*。

### 解题步骤

**第1步：计算所有点之间的欧几里得距离**

对于三维空间中两点 (x₁,y₁,z₁) 和 (x₂,y₂,z₂)，距离公式为： $$d = \sqrt{(x_2-x_1)^2 + (y_2-y_1)^2 + (z_2-z_1)^2}$$
![](附件/Pasted%20image%2020250615221035.png)

**第2步：对每个点，找出距离 ≤ 4 的邻居点**

| 样本  | p   |
| --- | --- |
| S1  | 1   |
| S2  | 4   |
| S3  | 3   |
| S4  | 2   |
| S5  | 4   |
| S6  | 2   |

**第3步：统计邻居数量，判断是否为噪声点**

如果某点的邻居数量 < 3，则该点为噪声点。

故，S1、S4、S6是噪声点


# 2. 卡方检验：关联分析消除冗余属性

## 基础概念

### 什么是卡方检验？

**卡方检验**是一种统计方法，用来检验两个分类变量之间是否相互独立。

**核心思想：**

- 如果两个变量独立 → 卡方值小 → 没有关联
- 如果两个变量相关 → 卡方值大 → 有显著关联

### 什么是冗余属性？

**冗余属性**是指对预测目标没有帮助或帮助很小的特征。

## 应用原理

在特征选择中，我们用卡方检验来测试：

> **"这个特征与目标变量是否有关联？"**

- **卡方值大** → 特征有用，保留
- **卡方值小** → 特征冗余，可以删除

## 形象例子：预测学生是否能考上大学

### 数据背景

我们有1000名学生的数据，想预测他们能否考上大学：

|特征|描述|
|---|---|
|学习成绩|优秀/良好/一般|
|鞋子尺码|36-44码|
|家庭收入|高/中/低|
|**目标变量**|是否考上大学（是/否）|

### 卡方检验过程

#### 特征1：学习成绩 vs 考上大学

**列联表：**

```
           考上大学    没考上    总计
优秀成绩      280       20      300
良好成绩      150      150      300  
一般成绩       70      330      400
总计         500      500     1000
```

**观察：** 学习成绩好的学生，考上大学的比例明显高 **卡方值：** 很大（比如 χ² = 156.25） **结论：** 学习成绩是**重要特征**，不冗余

#### 特征2：鞋子尺码 vs 考上大学

**列联表：**

```
           考上大学    没考上    总计
36-38码      125      125      250
39-41码      250      250      500
42-44码      125      125      250  
总计         500      500     1000
```

**观察：** 各种鞋码的学生，考上大学的比例都是50% **卡方值：** 接近0（χ² ≈ 0） **结论：** 鞋子尺码是**冗余特征**，应该删除

#### 特征3：家庭收入 vs 考上大学

**列联表：**

```
           考上大学    没考上    总计
高收入        200      100      300
中收入        200      200      400
低收入        100      200      300
总计         500      500     1000
```

**观察：** 家庭收入与考大学有一定关联 **卡方值：** 中等（比如 χ² = 33.33） **结论：** 家庭收入是**有用特征**，保留

---

**实际应用步骤**

1. **构建列联表**：特征值 × 目标变量
2. **计算卡方值**：χ² = Σ[(观察值-期望值)²/期望值]
3. **设定阈值**：比如 χ² < 3.84 认为是冗余
4. **特征筛选**：删除卡方值小的特征

---

**卡方统计量公式**

$$\chi^2 = \sum \frac{(O - E)^2}{E}$$

其中：

- **O** = 观察频数 (实际观察到的数值)
- **E** = 期望频数 (如果两变量独立时的理论数值)

### 计算步骤

*例子：学习成绩 vs 考上大学*

**步骤1：列联表（观察频数O）**

```
           考上大学    没考上    行总数
优秀成绩      280       20       300
良好成绩      150      150       300  
一般成绩       70      330       400
列总数       500      500      1000
```

**步骤2：计算期望频数E**

期望频数公式：$E = \frac{行总数 \times 列总数}{总样本数}$

按照比例✖样本数量=期望频数

**对于每个格子：**

1. **优秀成绩 & 考上大学**： $E_{11} = \frac{300 \times 500}{1000} = 150$
    
2. **优秀成绩 & 没考上**： $E_{12} = \frac{300 \times 500}{1000} = 150$
    
3. **良好成绩 & 考上大学**： $E_{21} = \frac{300 \times 500}{1000} = 150$
    
4. **良好成绩 & 没考上**： $E_{22} = \frac{300 \times 500}{1000} = 150$
    
5. **一般成绩 & 考上大学**： $E_{31} = \frac{400 \times 500}{1000} = 200$
    
6. **一般成绩 & 没考上**： $E_{32} = \frac{400 \times 500}{1000} = 200$
    

**期望频数表E：**

```
           考上大学    没考上    
优秀成绩      150      150      
良好成绩      150      150       
一般成绩      200      200      
```

**步骤3：计算(O-E)²/E**

|格子|O|E|O-E|(O-E)²|(O-E)²/E|
|---|---|---|---|---|---|
|优秀&考上|280|150|130|16,900|112.67|
|优秀&没考上|20|150|-130|16,900|112.67|
|良好&考上|150|150|0|0|0|
|良好&没考上|150|150|0|0|0|
|一般&考上|70|200|-130|16,900|84.5|
|一般&没考上|330|200|130|16,900|84.5|

**步骤4：求和得到卡方值**

$$\chi^2 = 112.67 + 112.67 + 0 + 0 + 84.5 + 84.5 = 394.34$$

### 结果解释

==判断标准==
- **卡方值 = 394.34**（很大！）
- **自由度** = (行数-1) × (列数-1) = (3-1) × (2-1) = 2
- **临界值**（α=0.05）≈ 5.99

==结论==: **394.34 >> 5.99** → 学习成绩与考上大学**高度相关**，不是冗余特征！

==更加直观理解：==

**卡方值的含义：**
- **卡方值大** → 观察值与期望值差距很大 → 变量间有强关联
- **卡方值小** → 观察值与期望值差不多 → 变量间相互独立

在我们的例子中：

- 如果成绩与考大学无关，我们期望优秀学生中有150人考上
- 但实际有280人考上（多了130人！）
- 这个巨大差异导致了很大的卡方值

这就是卡方检验发现"学习成绩不是冗余特征"的数学原理！


## 标准化概念

### 卡方独立性检验的基本框架

**假设设定：**

- H₀（零假设）：变量A和B相互独立
- H₁（备择假设）：变量A和B不独立（存在关联性）

### 检验统计量

卡方统计量的计算公式为： $$\chi^2 = \sum_{i=1}^{r}\sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

其中：

- Oᵢⱼ：观察频数（实际观测值）
- Eᵢⱼ：期望频数（理论期望值）
- 期望频数计算：$E_{ij} = \frac{第i行总数 \times 第j列总数}{总样本数}$

### 自由度

ppt中提到的自由度公式 **(r-1)×(c-1)** 是正确的：
- r：列联表的*行数*
- c：列联表的*列数*
- 自由度反映了在给定边际总数的情况下，可以自由变化的单元格数量

### 检验步骤

1. **构建列联表**：整理数据成r×c的交叉分类表
2. **计算期望频数**：基于独立性假设计算每个单元格的期望值
3. **计算卡方统计量**：使用上述公式
4. **确定临界值**：根据显著水平α和自由度查表
5. **做出判断**：比较统计量与临界值，或计算p值

### 显著水平的作用

显著水平α（常取0.05或0.01）决定了：

- 拒绝域的大小
- 犯第一类错误的概率
- 临界值的确定

**判断准则：**

- 如果 χ² > χ²ₐ(df)，则拒绝H₀，认为A、B不独立
- 如果 χ² ≤ χ²ₐ(df)，则不拒绝H₀，认为A、B独立

你在学习过程中有什么具体的疑问吗？比如实际计算步骤或者某个概念的理解？

# 3. 相关性分析（连续变量）

![](附件/Pasted%20image%2020250616222241.png)

Pearson相关系数是基于*标准差σ_A、σ_B* 和 *协方差Cov(A,B)* 的基础上进行计算的

根据PPT中的表格数据，我来演示Pearson相关系数和协方差这两个参数的计算过程：

## 数据整理

从表3.2可以看到：

- AllElectronics (A): 6, 5, 4, 3, 2
- HighTech (B): 20, 10, 14, 5, 5

## 第一步：计算基本统计量

**均值计算：**

- Ā = (6+5+4+3+2)/5 = 20/5 = 4
- B̄ = (20+10+14+5+5)/5 = 54/5 = 10.8

**标准差计算：**

- σ_A = √[Σ(aᵢ-Ā)²/n] = √[(2²+1²+0²+1²+2²)/5] = √[10/5] = √2 ≈ 1.414
- σ_B = √[Σ(bᵢ-B̄)²/n] = √[(9.2²+0.8²+3.2²+5.8²+5.8²)/5] = √[152.8/5] ≈ 5.528

## 第二步：计算协方差

使用公式：Cov(A,B) = Σ(aᵢ-Ā)(bᵢ-B̄)/n

|时间|aᵢ-Ā|bᵢ-B̄|(aᵢ-Ā)(bᵢ-B̄)|
|---|---|---|---|
|t1|2|9.2|18.4|
|t2|1|-0.8|-0.8|
|t3|0|3.2|0|
|t4|-1|-5.8|5.8|
|t5|-2|-5.8|11.6|

Cov(A,B) = (18.4-0.8+0+5.8+11.6)/5 = 35/5 = 7

## 第三步：计算Pearson相关系数

使用公式：r_{A,B} = Cov(A,B)/(σ_A × σ_B)

r_{A,B} = 7/(1.414 × 5.528) ≈ 7/7.817 ≈ 0.895

## 结果解释

- **协方差 = 7**：正值表示两个变量呈正相关趋势
- **相关系数 ≈ 0.895**：接近1，说明两只股票价格具有很强的正相关关系

这说明AllElectronics和HighTech的股票价格变动趋势非常相似。


# 3. 规范化

1. Min-Max规范化：线性映射到指定区间 
2. Z-score规范化：基于均值和标准差 
3. 小数定标：通过移动小数点位置规范化

这三种数据规范化方法各有特点，以下是详细解释：

## 1. Min-Max规范化（最小-最大规范化）

**原理：** 将数据线性缩放到指定区间，通常是[0,1]

**公式：**

```
新值 = (原值 - 最小值) / (最大值 - 最小值)
```

**举例：** 原数据：[10, 20, 30, 40, 50]

- 最小值=10，最大值=50
- 10 → (10-10)/(50-10) = 0
- 20 → (20-10)/(50-10) = 0.25
- 30 → (30-10)/(50-10) = 0.5
- 40 → (40-10)/(50-10) = 0.75
- 50 → (50-10)/(50-10) = 1

**特点：**

- ✅ 保持数据的原始分布形状
- ✅ 所有值都在指定范围内
- ❌ 对异常值敏感（一个极值会影响所有数据）

## 2. Z-score规范化（标准化）

**原理：** 将数据转换为均值=0，标准差=1的标准正态分布

**公式：**

```
新值 = (原值 - 均值) / 标准差
```

**举例：** 原数据：[10, 20, 30, 40, 50]

- 均值=30，标准差≈15.81
- 10 → (10-30)/15.81 = -1.27
- 20 → (20-30)/15.81 = -0.63
- 30 → (30-30)/15.81 = 0
- 40 → (40-30)/15.81 = 0.63
- 50 → (50-30)/15.81 = 1.27

**特点：**

- ✅ 对异常值相对不敏感
- ✅ 适合正态分布的数据
- ✅ 数据范围不固定，但集中在[-3,3]区间
- ❌ 需要知道数据的统计特性

## 3. 小数定标规范化

**原理：** 通过移动小数点位置，使所有数据的绝对值都小于1

**公式：**

```
新值 = 原值 / 10^k
```

其中k是使得max(|新值|) < 1的最小整数

**举例：** 原数据：[123, 456, 789, -234]

- 最大绝对值=789，需要移动3位小数点(k=3)
- 123 → 123/1000 = 0.123
- 456 → 456/1000 = 0.456
- 789 → 789/1000 = 0.789
- -234 → -234/1000 = -0.234

**特点：**

- ✅ 计算简单
- ✅ 保持数据的符号和相对大小
- ✅ 所有值都在[-1,1]范围内
- ❌ 可能损失精度
- ❌ 不考虑数据分布特征

## 使用场景对比

|方法|适用场景|输出范围|对异常值敏感度|
|---|---|---|---|
|Min-Max|已知数据范围，需要固定区间|[0,1]或指定区间|高|
|Z-score|数据近似正态分布，机器学习|无固定范围，通常[-3,3]|中|
|小数定标|数据量级相差很大，简单处理|[-1,1]|低|

**选择建议：**

- **机器学习算法**：通常用Z-score
- **神经网络**：通常用Min-Max到[0,1]
- **快速预处理**：可以用小数定标


# 4. 分箱

分箱（Binning）是数据预处理中的一种**数据离散化**技术，将连续数值转换为分类数据。这两种方法各有特点：

## 1. 等宽分箱（距离划分）

**原理：** 将数据范围等分为==N个相等宽度== （数值意义上的）的区间

**公式：**

```
箱宽度 = (最大值 - 最小值) / N
```

**举例：** 数据：[1, 3, 5, 7, 9, 15, 20, 25, 30, 35]，分成5个箱

- 范围：1-35，箱宽度 = (35-1)/5 = 6.8
- 箱1：[1, 7.8) → {1, 3, 5, 7}
- 箱2：[7.8, 14.6) → {9}
- 箱3：[14.6, 21.4) → {15, 20}
- 箱4：[21.4, 28.2) → {25}
- 箱5：[28.2, 35] → {30, 35}

**优点：**

- ✅ 计算简单直接
- ✅ 箱边界清晰易理解

**缺点：**

- ❌ **异常值主导性能**：一个极值会影响所有箱的划分
- ❌ **倾斜数据处理不好**：数据分布不均时，某些箱可能为空或数据过多

## 2. 等频分箱/等深分箱（频率划分）

**原理：** 每个箱包含大约相同数量的数据点

**举例：** 数据：[1, 3, 5, 7, 9, 15, 20, 25, 30, 35]（10个点），分成5个箱，每箱2个点

- 箱1：{1, 3}
- 箱2：{5, 7}
- 箱3：{9, 15}
- 箱4：{20, 25}
- 箱5：{30, 35}

**优点：**

- ✅ **良好的数据扩展**：每个箱都有足够的样本
- ✅ 对数据分布不敏感
- ✅ 统计分析更稳定

**缺点：**

- ❌ **分类变量可能很赖手**：箱边界可能不直观，难以解释

## 实际应用场景

### 年龄分箱示例

**等宽分箱：**

```
0-20岁，21-40岁，41-60岁，61-80岁
```

- 直观易懂，但可能某个年龄段人数很少

**等频分箱：**

```
根据人数分：每组25%的人
可能是：0-25岁，26-35岁，36-50岁，51-80岁
```

- 每组样本充足，但年龄段不规整

### 收入分箱示例

**原始数据：** [2万, 3万, 4万, 5万, 6万, 50万, 100万]

**等宽分箱：**

- 被50万、100万两个高收入拉开，大部分人都挤在第一个箱里

**等频分箱：**

- 每个箱人数相等，更适合分析不同收入群体的特征

## 分箱的目的

1. **简化模型**：减少特征的复杂度
2. **处理异常值**：将极值归入区间，减少影响
3. **捕获非线性关系**：将连续变量转为分类变量
4. **便于业务理解**：如"低收入"、"中收入"、"高收入"

**选择建议：**

- **数据分布均匀**：用等宽分箱
- **数据有偏斜或异常值**：用等频分箱
- **需要业务解释性**：用等宽分箱
- **需要统计稳定性**：用等频分箱


----
---
---

# 以下为补充：

下面是补充讲解PPT中的概念

---
---
---

# 1)数据集成、数据规约和数据变换

数据集成、数据规约和数据变换是数据预处理的三个核心步骤，它们构成了一个**递进式的处理流程**。让我详细解释它们的关系：

## 完整的数据预处理流程

```
原始数据源 → 数据集成 → 数据清洗 → 数据规约 → 数据变换 → 可用数据
```

## 三者的具体作用和关系

### 1. 数据集成 (Data Integration)

**作用：** 解决"**数据从哪来**"的问题

- 将多个异构数据源合并
- 解决数据源之间的冲突和不一致

**输入：** 多个分散的数据源 **输出：** 统一的完整数据集

### 2. 数据规约 (Data Reduction)

**作用：** 解决"**要多少数据**"的问题

- 在保持信息完整性的前提下减少数据量
- 提高后续处理效率

**输入：** 集成后的大型数据集 **输出：** 精简但信息完整的数据集

### 3. 数据变换 (Data Transformation)

**作用：** 解决"**数据如何用**"的问题

- 将数据转换为适合分析算法的格式
- 改善数据质量和可用性

**输入：** 规约后的数据集 **输出：** 符合算法要求的最终数据集

## 三者关系的形象比喻

**建房子的过程：**

```
数据集成 = 收集建材
• 从不同供应商收集砖头、水泥、钢筋等
• 确保材料规格统一，质量一致

数据规约 = 选择必要建材  
• 根据房屋设计图，选择必需的材料
• 去掉多余的材料，节省空间和成本

数据变换 = 加工建材
• 将原材料加工成可直接使用的形式
• 切割钢筋、调配混凝土等
```

## 实际案例演示

**电商用户分析项目：**

### 第1步：数据集成

```
用户基本信息表 + 订单记录表 + 浏览行为表 + 评价表
↓
统一的用户全景数据表
- 解决：用户ID不统一、字段命名冲突、数据格式不一致
```

### 第2步：数据规约

```
原始数据：100万用户，200个字段，5年历史数据
↓  
精简数据：活跃用户50万，关键字段30个，近1年数据
- 解决：数据量过大、存储成本高、处理速度慢
```

### 第3步：数据变换

```
精简数据 → 机器学习可用数据
- 年龄 → 年龄段分组 
- 购买金额 → 标准化[0,1]
- 文本评价 → 情感得分
- 解决：数据格式不符合算法要求
```

## 为什么按这个顺序？

### 1. 数据集成必须在前

- **原因：** 必须先有完整的数据，才能判断哪些数据重要
- **如果颠倒：** 可能丢失重要信息源

### 2. 数据规约在变换前

- **原因：** 先减少数据量，再进行复杂变换，提高效率
- **如果颠倒：** 对大量无用数据进行昂贵的变换操作

### 3. 数据变换在最后

- **原因：** 变换往往是针对特定算法的，需要在确定最终数据集后进行
- **时机：** 已知要用什么算法，需要什么格式

## 三者的依赖关系

```
数据集成 ← 前提条件 ← 数据规约 ← 前提条件 ← 数据变换
   ↓           ↓           ↓
提供原料     提供素材     提供成品
```

**相互依赖：**

- 数据规约**依赖**数据集成的结果
- 数据变换**依赖**数据规约的结果
- 每一步都为下一步**奠定基础**

## 质量递进关系

```
数据完整性 → 数据精简性 → 数据适用性
     ↑           ↑           ↑
   数据集成    数据规约    数据变换
```

## 实际项目中的考虑

**并非严格线性：**

- 有时需要**迭代优化**
- 可能在数据变换后发现需要**回到规约阶段**
- 实际项目中这三步可能**交替进行**

**但基本原则不变：**

1. 先解决**数据来源**问题（集成）
2. 再解决**数据量**问题（规约）
3. 最后解决**数据格式**问题（变换）

这三个步骤共同构成了从"原始数据"到"可分析数据"的完整转换链条，缺一不可，相互支撑！