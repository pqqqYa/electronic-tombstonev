
## 1 数据挖掘与知识发现之间的关系(教材1.2）。

KDD过程包括数据清理、数据集成、数据选择、数据转换、 数据挖掘、模式评估和知识表示

## 2 数据挖掘的主要功能（教材1.2）？

特征描述、鉴别、关联、分类、聚类、趋势和离群点分析

## 3 数据挖掘标准流程（参考ppt中的流程图）？

![550](attachment/数据挖掘标准流程.jpeg)

## 4 数据探索的意义及其主要内容（教材第二章）。

数据探索的意义

1. 有助于完成数据挖掘过程中第一个主要步骤：数据预处理
2. 有助于理解数据、洞悉数据甚至简化其后的数据分析任务

主要内容

1. 了解数据由什么类型的属性或字段组成？每个属性具有何种类型的数据值，哪些是离散的，哪些是连续的。
2. 数据看上去如何？值如何分布？是否存在脏数据？能否看出离群点？
3. 各个属性之间有什么样的关联性？

## 5 数据对象和属性的基本概念（教材2.1或ppt内容）

- 数据集由数据对象组成，一个数据对象代表着一个实体。
- 数据库的行对应于数据对象，列对应于属性
- 通常数据对象用属性描述，也被称为样本、实例、元组、数据点或对象。
- 属性是一个数据字段，表示数据对象的一个特征，也被称为维，特征和变量


## 6 数据的基本统计描述方法有哪些？特别是数据的集中趋势度量，四分位数. 极差等计算（教材2.2）？

### 6.1 中心趋势度量

![](attachment/Pasted%20image%2020250620172129.png)

### 6.2 四分位数

1. 四分位数是3个值，把排序的数据集划分成4个相等的部分。
2. 四分位数: Q1 (第一个四分位数), Q3 (第三个四分位数) 
3. 四分位极差: IQR = Q3 – Q1 
4. 五数概括: 最小值,四分位数 Q1, 中位数,四分位数 Q3, 最大值 

### 6.3 极差

设$x_1,x_2,\cdots,x_N$是某数值属性$X$上的观测的集合。该集合的极差（range）是最大值$\max(x_i)$与最小值$\min(x_i)$之差。


## 7 常见的几种相似性度量方式的计算，特别是不同类型的属性相似性的度量（教材2.4）



### 7.1 欧几里得距离

最流行的距离度量，用于计算数值属性刻画的对象间的相异性，是二维和三维空间中两点之间的真实距离。公式为$$d(i,j)=\sqrt{\sum_{k=1}^{p}(x_{ik}-x_{jk})^2}$$其中$i=(x_{i1},x_{i2},\cdots,x_{ip})$和$j=(x_{j1},x_{j2},\cdots,x_{jp})$是两个被P个数值属性描述的对象。

### 7.2 曼哈顿距离

又称城市块距离，是城市两点之间的街区距离。定义为$d(i,j)=\sum_{k=1}^{p}|x_{ik}-x_{jk}|$常用于计算数值属性对象间的相异性。

### 7.3 闵可夫斯基距离

是欧几里得距离和曼哈顿距离的推广，公式为$d(i,j)=\sqrt[h]{\sum_{k=1}^{p}|x_{ik}-x_{jk}|^h}$其中$h\geq1$。当$p=1$时表示曼哈顿距离，$p=2$时表示欧几里得距离。

### 7.4 上确界距离（切比雪夫距离）

$h\rightarrow\infty$时闵可夫斯基距离的推广，计算方法是找出属性f产生两个对象的最大值差，即$$d(i,j)=\lim_{h\rightarrow\infty}\left(\sum_{f=1}^{p}|x_{if}-x_{jf}|^h\right)^{\frac{1}{h}}=\max_{f}|x_{if}-x_{jf}|$$

### 7.5 余弦相似度

常用于比较文档或针对给定的查询词向量对文档排序，通过计算两个向量夹角的余弦值来评估相似度。公式为$$sim(x,y)=\frac{x\cdot y}{\|x\|\|y\|}$$其中$\|x\|$和$\|y\|$分别是向量x和y的欧几里得范数。

### 7.6 标称属性的邻近性度量

两个对象i和j之间的相异性根据不匹配率计算，公式为$$d(i,j)=\frac{p-m}{p}$$其中m是匹配的数目，P是刻画对象的属性总数。相似性可用$$sim(i,j)=1-d(i,j)=\frac{m}{p}$$计算。

### 7.7 二元属性的邻近性度量

对称二元属性：相异性公式为$$d(i,j)=\frac{q+s}{q+r+s+t}$$其中q是对象i和j都取1的属性数，r是在对象i中取1、在对象j中取0的属性数，s是在对象i中取0、在对象j中取1的属性数，t是对象i和j都取0的属性数。  
非对称二元属性：相异性公式为$$d(i,j)=\frac{r+s}{q+r+s+t}$$相似性可用Jaccard系数计算，即$$sim(i,j)=\frac{q}{q+r+s}=1-d(i,j)$$

### 7.8 序数属性的邻近性度量

计算时先将序数属性的值替换为对应的排位$r_{if}$，再将其值域映射到$[0.0,1.0]$上进行数据规格化，用$$z_{if}=\frac{r_{if}-1}{M_f-1}$$代替$r_{if}$，最后用数值属性的距离度量方法计算相异性。

### 7.9 混合类型属性的相异性

将所有属性类型一起处理，把有意义的属性转换到共同区间$[0.0,1.0]$上，对象i和j之间的相异性$d(i,j)$定义为$$d(i,j)=\frac{\sum_{k=1}^{p}\delta_{ij}^{(k)}d_{ij}^{(k)}}{\sum_{k=1}^{p}\delta_{ij}^{(k)}}$$其中$\delta_{ij}^{(k)}$是指示符，$d_{ij}^{(k)}$根据属性类型计算。

## 8 K-means算法的基本原理及其优缺点。

基本原理

k-means算法通过迭代优化（而非评估不同分区）将包含 $n$ 个对象的数据集 $D$ ​**预设**划分为 k 个簇，其核心目标是**最小化平方误差和（SSE）​**​：算法首先随机初始化 $k$ 个质心，随后交替执行 _(1) 将对象分配给最近质心形成簇_ 和 _(2) 根据簇内对象重新计算质心_ 的步骤直至收敛，最终每个簇由其质心（簇内点的几何均值）表示，并使得目标函数 $\sum_{i=1}^{k}\sum_{x \in C_i} \| x - \mu_i \|^2$ 达到局部最小，其中 $\mu_i$ 是簇 $C_i$ 的质心，$k$ 为用户预先指定的参数，但存在陷入局部最优解的隐患。

优点:

1. 擅长处理球状分布的数据，当结果聚类是密集的，而且类和类之间的区别比较明显时，k-均值聚类算法的效果比较好。
2. 对于处理大数据集，是相对可伸缩的和高效的，它的复杂度是O(nkt),n是对象的个数，k是簇的数目，t是迭代的次数。
3. 相比其他的聚类算法，k-均值聚类算法比较简单、容易掌握。

缺点:

1. 需要预先指定集群的数量k(有自动确定最佳k的方法)
2. 对噪声和离群点敏感
3. 不适合发现非凸形状的簇

## 9 数据预处理的主要任务其基本步骤（教材3.1）


### 9.1 数据清洗

- **任务**：处理数据中的缺失值、噪声、异常值和不一致数据，提高数据的准确性和完整性。

- **基本步骤**：

	1. 缺失值处理
	2. 忽略元组：当缺失值出现在类标签或元组有多个属性缺失时使用，但可能导致数据丢失。
	3. 人工填写缺失值：适用于少量重要数据缺失的情况，但费时费力且可行性低。
	4. 填充缺失值：
	5. 使用常量填充，如为分类变量新增 “unknown” 类别。
	6. 使用属性的中心度量（均值、中位数、众数等）填充。
	7. 使用与给定元组同一类的所有样本的属性均值或中位数等。
	8. 通过回归、贝叶斯公式或决策树推断最可能的值。
	9. 噪声数据处理
	10. 分箱：将数据排序并划分到等频率的箱子中，通过分箱的均值、中值或边界值进行平滑。
	11. 回归：将数据拟合到回归函数中使其平滑。
	12. 离群点分析：通过聚类、统计学假设检验或人工检测等方法识别并处理异常值。
	13. 不一致数据处理：利用数据库系统保护数据一致性，在数据集成过程中解决编码、度量和多个系统不同步等导致的不一致问题。

### 9.2 数据集成

- **任务**：将来自多个源的异构数据合并到一个一致的存储中（如数据仓库），解决数据属性和对象识别、数据值冲突等问题。

- **基本步骤**

	1. 数据属性识别：集成来自不同来源的元数据，解决属性命名不一致的问题，例如 “customer_id” 和 “cust_number” 的统一。
	2. 数据对象识别：从多个数据源识别现实世界中的实体，处理不同名称表示同一实体的情况，如 “Bill Clinton” 和 “William Clinton”。
	3. 检测和解决数据值冲突：分析同一实体在不同来源中属性值的差异，解决因表示方式或尺度不同（如公制与英制单位）导致的冲突。
	4. 处理数据集成中的冗余：通过相关分析和协方差分析检测冗余属性，了解业务并仔细整合数据以减少或避免冗余和不一致性，提高挖掘速度。

### 9.3 数据归约

- **任务**：在保持数据原貌特征的前提下，最大限度地精简数据量，提高挖掘效率。

- **基本步骤**

- **维归约（降维）**

- **小波变换**：通过小波变换进行数据压缩，减少属性个数。

- **主成分分析（PCA）**：将多个变量转换为少数几个综合变量，保留主要信息。

- **特性子集选择**：采用逐步向前选择、逐步向后删除、两者组合或决策树归纳等启发式方法选择重要属性，删除不必要的特征。

- **数量归约**

- **回归和对数线性模型**：用模型表示数据，减少数据量。

- **直方图**：将数据分布划分为不相交的子集或桶，采用等宽或等频划分方法。

- **抽样**：使用简单随机抽样（无放回或放回）、分层抽样等方法，用较小的随机样本表示大的数据集。

- **数据立方体聚合**：对数据进行汇总和聚集，减少数据量。

- **数据压缩**：采用有损或无损压缩技术对数据进行压缩。

### 9.4 数据变换与数据离散化

- **任务**：对数据进行转换，使其更适合挖掘和分析，包括平滑、构造新属性、聚合、规范化、离散化和概念分层等。

- **基本步骤**

- **平滑**：去除数据中的噪音，如使用分箱、回归等方法。

- **属性 / 特征构造**：根据给定的属性构造新的属性，以更好地反映数据特征。

- **聚合**：对数据进行汇总或聚集，如计算总和、平均值等。

- **规范化**

- **最小 - 最大规范化**：将数据线性变换到指定的范围 [new_minA, new_maxA]。

- **Z-score 规范化**：基于数据的均值和标准差进行规范化。

- **小数定标**：通过移动小数点的位置对数据进行规范化，使 Max (|ν’|) < 1。

- **离散化**：将连续属性的范围划分为区间，使用区间标签替换实际数据值，减少数据大小。

- **分箱（无监督）**：采用等宽或等频分箱方法，递归地对属性执行分割或合并。

- **直方图（无监督）**：基于直方图进行自顶向下分裂。

- **聚类分析（无监督）**：通过聚类进行自顶向下分裂或自底向上融合。

- **决策树（有监督）**：利用信息熵确定分裂点，自上而下递归分割。

- **相关分析（如 χ2 分布离散化）（无监督）**：基于类信息自底向上合并相邻区间。

- **概念分层**：将较低级的概念替换为较高级的概念，形成概念分层结构，便于在不同粒度查看数据，可由领域专家显式指定或自动生成。

## 10 数据预处理的意义与目的（教材3.1）

## 11 常见的缺失值处理手段有哪些（教材3.2.1）

## 12 常见的噪声处理手段有哪些，分箱的意义与类型？（教材2.4）

## 13 抽样是数据归约技术的一种，常见的抽样方式有哪些（教材3.4.8）

## 14 数据质量通常涉及的因素有哪些。

## 15 常用的数据变换策略有哪些？

## 16 数据规范化的常用方法，特别是计算案例学习（教材3.5）

## 17 关联规则挖掘的意义与步骤，什么是先验性质以及其意义？（教材6）

## 18 apriori算法的基本流程，及具体案例分析？

## 19 FP-group算法的算法原理及基本流程。

## 20 ID3算法的基本流程与步骤，参照ppt掌握决策树的生成过程。

## 21 分类与聚类的区别？

## 22 分类模型评估指标的度量方式及具体计算过程，结合例题进行分析，要求能够掌握计算公式。

## 23 特征工程中新增特征的方法

## 24 朴素bayes算法的原理及实现过程（参考ppt）

## 25 实验中常用包的作用和意义。

## 26 常见的分类. 聚类. 关联分析方法有哪些？

## 27 论述题主要是考察大家的数据挖掘思维（类似实验汇报）

PS：复习方式：计算题多复习书本和ppt例题加深算法理解。