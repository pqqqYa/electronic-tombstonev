
##### 1. 数据挖掘与知识发现之间的关系(教材1.2）。

KDD过程包括数据清理、数据集成、数据选择、数据转换、 数据挖掘、模式评估和知识表示

##### 2. 数据挖掘的主要功能（教材1.2）？

特征描述、鉴别、关联、分类、聚类、趋势和离群点分析

##### 3. 数据挖掘标准流程（参考ppt中的流程图）？

![550](attachment/数据挖掘标准流程.jpeg)

##### 4. 数据探索的意义及其主要内容（教材第二章）。

数据探索的意义

1. 有助于完成数据挖掘过程中第一个主要步骤：数据预处理
2. 有助于理解数据、洞悉数据甚至简化其后的数据分析任务

主要内容

1. 了解数据由什么类型的属性或字段组成？每个属性具有何种类型的数据值，哪些是离散的，哪些是连续的。
2. 数据看上去如何？值如何分布？是否存在脏数据？能否看出离群点？
3. 各个属性之间有什么样的关联性？

##### 5. 数据对象和属性的基本概念（教材2.1或ppt内容）

- 数据集由数据对象组成，一个数据对象代表着一个实体。
- 数据库的行对应于数据对象，列对应于属性
- 通常数据对象用属性描述，也被称为样本、实例、元组、数据点或对象。
- 属性是一个数据字段，表示数据对象的一个特征，也被称为维，特征和变量


##### 6. 数据的基本统计描述方法有哪些？特别是数据的集中趋势度量，四分位数. 极差等计算（教材2.2）？

###### 中心趋势度量

![](attachment/Pasted%20image%2020250620172129.png)

###### 四分位数

1. 四分位数是3个值，把排序的数据集划分成4个相等的部分。
2. 四分位数: Q1 (第一个四分位数), Q3 (第三个四分位数) 
3. 四分位极差: IQR = Q3 – Q1 
4. 五数概括: 最小值,四分位数 Q1, 中位数,四分位数 Q3, 最大值 

###### 极差

设$x_1,x_2,\cdots,x_N$是某数值属性$X$上的观测的集合。该集合的极差（range）是最大值$\max(x_i)$与最小值$\min(x_i)$之差。


##### 7. 常见的几种相似性度量方式的计算，特别是不同类型的属性相似性的度量（教材2.4）


###### 欧几里得距离
最流行的距离度量，用于计算数值属性刻画的对象间的相异性，是二维和三维空间中两点之间的真实距离。公式为$$d(i,j)=\sqrt{\sum_{k=1}^{p}(x_{ik}-x_{jk})^2}$$其中$$i=(x_{i1},x_{i2},\cdots,x_{ip})$$和$$j=(x_{j1},x_{j2},\cdots,x_{jp})$$是两个被P个数值属性描述的对象。

###### 曼哈顿距离
又称城市块距离，是城市两点之间的街区距离。定义为$$d(i,j)=\sum_{k=1}^{p}|x_{ik}-x_{jk}|$$常用于计算数值属性对象间的相异性。

###### 闵可夫斯基距离
是欧几里得距离和曼哈顿距离的推广，公式为$$d(i,j)=\sqrt[h]{\sum_{k=1}^{p}|x_{ik}-x_{jk}|^h}$$其中$$h\geq1$$。当$p=1$时表示曼哈顿距离，$p=2$时表示欧几里得距离。

###### 上确界距离（切比雪夫距离）
是$$h\rightarrow\infty$$时闵可夫斯基距离的推广，计算方法是找出属性f产生两个对象的最大值差，即$$d(i,j)=\lim_{h\rightarrow\infty}\left(\sum_{f=1}^{p}|x_{if}-x_{jf}|^h\right)^{\frac{1}{h}}=\max_{f}|x_{if}-x_{jf}|$$

###### 余弦相似度
常用于比较文档或针对给定的查询词向量对文档排序，通过计算两个向量夹角的余弦值来评估相似度。公式为$$sim(x,y)=\frac{x\cdoty}{\|x\|\|y\|}$$其中$\|x\|$和$\|y\|$分别是向量x和y的欧几里得范数。

###### 标称属性的邻近性度量
两个对象i和j之间的相异性根据不匹配率计算，公式为$$d(i,j)=\frac{p-m}{p}$$其中m是匹配的数目，P是刻画对象的属性总数。相似性可用$$sim(i,j)=1-d(i,j)=\frac{m}{p}$$计算。

###### 二元属性的邻近性度量
对称二元属性：相异性公式为$$d(i,j)=\frac{q+s}{q+r+s+t}$$其中q是对象i和j都取1的属性数，r是在对象i中取1、在对象j中取0的属性数，s是在对象i中取0、在对象j中取1的属性数，t是对象i和j都取0的属性数。  
非对称二元属性：相异性公式为$$d(i,j)=\frac{r+s}{q+r+s+t}$$相似性可用Jaccard系数计算，即$$sim(i,j)=\frac{q}{q+r+s}=1-d(i,j)$$

###### 序数属性的邻近性度量
计算时先将序数属性的值替换为对应的排位$r_{if}$，再将其值域映射到$[0.0,1.0]$上进行数据规格化，用$$z_{if}=\frac{r_{if}-1}{M_f-1}$$代替$r_{if}$，最后用数值属性的距离度量方法计算相异性。

###### 混合类型属性的相异性
将所有属性类型一起处理，把有意义的属性转换到共同区间$[0.0,1.0]$上，对象i和j之间的相异性$d(i,j)$定义为$$d(i,j)=\frac{\sum_{k=1}^{p}\delta_{ij}^{(k)}d_{ij}^{(k)}}{\sum_{k=1}^{p}\delta_{ij}^{(k)}}$$其中$\delta_{ij}^{(k)}$是指示符，$d_{ij}^{(k)}$根据属性类型计算。

##### 8. K-means算法的基本原理及其优缺点。

基本原理

- 构造不同的分区，然后根据一些标准对它们进行评估
将一个包含n个对象的数据集D划分为k个簇，使距离平方和最小化(其中$c_i$是簇$c_i$的质心或中心)，每个簇用簇的重心(簇的平均值) 表示，即为k


优点:

1. 擅长处理球状分布的数据，当结果聚类是密集的，而且类和类之间的区别比较明显时，k-均值聚类算法的效果比较好。
2. 对于处理大数据集，是相对可伸缩的和高效的，它的复杂度是O(nkt),n是对象的个数，k是簇的数目，t是迭代的次数。
3. 相比其他的聚类算法，k-均值聚类算法比较简单、容易掌握。

缺点:

1. 需要预先指定集群的数量k(有自动确定最佳k的方法)
2. 对噪声和离群点敏感
3. 不适合发现非凸形状的簇

##### 9. 数据预处理的主要任务其基本步骤（教材3.1）

##### 10. 数据预处理的意义与目的（教材3.1）

##### 11. 常见的缺失值处理手段有哪些（教材3.2.1）

##### 12. 常见的噪声处理手段有哪些，分箱的意义与类型？（教材2.4）

##### 13. 抽样是数据归约技术的一种，常见的抽样方式有哪些（教材3.4.8）

##### 14. 数据质量通常涉及的因素有哪些。

##### 15. 常用的数据变换策略有哪些？

##### 16. 数据规范化的常用方法，特别是计算案例学习（教材3.5）

##### 17. 关联规则挖掘的意义与步骤，什么是先验性质以及其意义？（教材6）

##### 18. apriori算法的基本流程，及具体案例分析？

##### 19. FP-group算法的算法原理及基本流程。

##### 20. ID3算法的基本流程与步骤，参照ppt掌握决策树的生成过程。

##### 21. 分类与聚类的区别？

##### 22. 分类模型评估指标的度量方式及具体计算过程，结合例题进行分析，要求能够掌握计算公式。

##### 23. 特征工程中新增特征的方法

##### 24. 朴素bayes算法的原理及实现过程（参考ppt）

##### 25. 实验中常用包的作用和意义。

##### 26. 常见的分类. 聚类. 关联分析方法有哪些？

##### 27. 论述题主要是考察大家的数据挖掘思维（类似实验汇报）

PS：复习方式：计算题多复习书本和ppt例题加深算法理解。