深度学习的基础：神经网络，通过构建多层深度神经网络来模拟人类大脑的信息处理与学习机制


# Pytorch解决问题的流程

以手写识别为例子

准备数据

导入不同的数据集

定义模型

Convolution卷积层：图像变成0-1之间的数据矩阵
Subsampling池化层：把原来很大的图像矩阵压缩，变成一张更小更容易计算的图像矩阵
Full connection全连接层：把池化层中的数据展开成一段数据，这段数据也作为了图片的”身份证“

训练模型

将训练模型数据输入模型，计算损失并调整更新模型参数，重复多个周期，指导模型很好的学到了训练数据里面的特征

大量已经标定的数据输入计算机，在不断的学习复习的过程

损失函数：衡量模型预测的准确性（交叉熵损失(Cross Entropy Loss)等）
优化器：用于调整模型的参数（随机梯度下降(SGD)等）

评估模型

预测

随着一系列步骤精炼和提取图像特征，随着数据量的减少，特征的精确度逐渐增高

# 神经网络的架构

input → Weights → Summation and Bias → Activation → Output

* input输入
* Weights权重
* Summation and Bias每个数据与不同权重相乘后加和偏差
* Activation激活函数
* Output

当损失函数收敛到最小后得到最终结果

# 卷积神经网络

一种专为图像输入而设计的神经网络

# Pytorch实操步骤

## 1. 准备数据

数据下载，数据格式转换，数据集划分：准备数据，将数据变成适合神经网络训练的格式

加载数据

### 1.1 转换数据格式

最初是以PIL图像格式存在，通常是像素数组的形式，每个像素包含了图像的一部分信息，比如颜色信息，这些信息可以是彩色的包含RGB通道，或者是单色的灰度图像。

为了更方便的使用，需要将这些图像转换为PyTorch Tensor图像，把0-255转换为0-1之间的浮点数。

### 1.2 数据集划分

每次遍历整个数据集会导致内存不足的问题，深度学习中需要将其分解为更小的batch，即将所有的总数据集分成了一块一块的子数据集。

这样可以快速多次的对模型进行调整，而不是等很长时间进行一次大的调整。

### 1.3 可视化训练集示例

随机抽取一些进行展示

## 2. 特征提取

### 2.1 卷积

卷积是一种线性运算，将一组权重与输入相乘生成新的二位数组

卷积层数增多就可以提高更高级的特征

### 2.2 池化

缩小表示空间的大小，提高计算效率

最大池化：捕捉数组的最大值，减少计算所需的值的数量

### 2.3 代码解读

定义两个卷积层和两个池化层

~~~python
# 模型定义
class CNN(Module):
    # 定义模型属性
    def __init__(self, n_channels):
        super(CNN, self).__init__()
        # 输入到卷积层 1
        self.hidden1 = Conv2d(n_channels, 32, (3,3))
        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')
        self.act1 = ReLU() 
        # 池化层 1
        self.pool1 = MaxPool2d((2,2), stride=(2,2))
~~~


**I. 定义这个方法，后面直接使用即可**

`class CNN(Module):`定义一个名为CNN的类，继承自torch.nn.Module，是PyTorch中所有神经网络模块的[基类（父类）](../面向对象程序设计/基类（父类）.md)

`def __init__(self, n_channels):`CNN类的构造函数，包含2个参数，`self`代表地址，一般不会修改，`n_channels`代表输入图像的[通道](../计算机视觉/通道.md)数

`super(CNN, self).__init__()`调用父类Module的初始化方法，确保正确的初始化模型

**II. 卷积层1**
`self.hidden1 = Conv2d(n_channels, 32, (3,3))`第一个卷积层接收`n_channels`个输入通道，并输出32个特征图（输出通道数，即这里有32个卷积层用来处理训练数据集，每个[卷积核](卷积核.md)可以提取一种特征，最终得到32个映射），使用的是3×3的[卷积核](卷积核.md)进行卷积运算；`Conv2d`是PyTorch中用于创建二维卷积层的类

`kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')`使用`kaiming_uniform_`初始化权重并设置[激活函数](激活函数.md)为ReLu，深度学习模型训练过程的本质是对weight（即参数）进行更新，这需要每个参数有对应的初始值，`self.hidden1.weight`表示卷积层1的[Tensor张量](Tensor张量.md)

`self.act1 = ReLU() `创建一个ReLU[激活函数](激活函数.md)并将其赋值给act1属性

**III. 池化层1**

`self.pool1 = MaxPool2d((2,2), stride=(2,2))`将输入的特征图分割成(2，2)大小的区域，并从每个区域中去最大值，`stride=(2,2)`是步长，决定了[池化核](池化核.md)在输入特征图上的滑动速度。

通过池化，在保留最重要特征的同时，图的尺寸再一次缩小，减少了模型的计算量和参数数量，同时增加在不同模型之间的通用性（减少过拟合，提高泛化能力）

![](附件/池化层.png)