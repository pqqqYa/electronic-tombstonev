深度学习的基础：神经网络，通过构建多层深度神经网络来模拟人类大脑的信息处理与学习机制


# Pytorch解决问题的流程

以手写识别为例子

准备数据

导入不同的数据集

定义模型

Convolution卷积层：图像变成0-1之间的数据矩阵
Subsampling池化层：把原来很大的图像矩阵压缩，变成一张更小更容易计算的图像矩阵
Full connection全连接层：把池化层中的数据展开成一段数据，这段数据也作为了图片的”身份证“

训练模型

将训练模型数据输入模型，计算损失并调整更新模型参数，重复多个周期，指导模型很好的学到了训练数据里面的特征

大量已经标定的数据输入计算机，在不断的学习复习的过程

损失函数：衡量模型预测的准确性（交叉熵损失(Cross Entropy Loss)等）
优化器：用于调整模型的参数（随机梯度下降(SGD)等）

评估模型

预测

随着一系列步骤精炼和提取图像特征，随着数据量的减少，特征的精确度逐渐增高

# 神经网络的架构

input → Weights → Summation and Bias → Activation → Output

* input输入
* Weights权重
* Summation and Bias每个数据与不同权重相乘后加和偏差，权重矩阵乘输入数据加偏置项$z = W · x +b$
* Activation激活函数
* Output

当损失函数收敛到最小后得到最终结果

# 卷积神经网络

一种专为图像输入而设计的神经网络

一般模型训练的全部过程
1. 数据收集
2. 初始化
3. 前向传播
4. 计算误差
5. 反向传播和参数更新
6. 重复训练

# 单层CNN网络的构建

## 1. 准备数据

数据下载，数据格式转换，数据集划分：准备数据，将数据变成适合神经网络训练的格式

加载数据

### 1.1 转换数据格式

最初是以PIL图像格式存在，通常是像素数组的形式，每个像素包含了图像的一部分信息，比如颜色信息，这些信息可以是彩色的包含RGB通道，或者是单色的灰度图像。

为了更方便的使用，需要将这些图像转换为PyTorch Tensor图像，把0-255转换为0-1之间的浮点数。

### 1.2 数据集划分

每次遍历整个数据集会导致内存不足的问题，深度学习中需要将其分解为更小的batch，即将所有的总数据集分成了一块一块的子数据集。

这样可以快速多次的对模型进行调整，而不是等很长时间进行一次大的调整。

### 1.3 可视化训练集示例

随机抽取一些进行展示

## 2. 特征提取

### 2.1 卷积

卷积是一种线性运算，将一组权重与输入相乘生成新的二位数组

卷积层数增多就可以提高更高级的特征

### 2.2 池化

缩小表示空间的大小，提高计算效率

最大池化：捕捉数组的最大值，减少计算所需的值的数量

### 2.3 代码解读

定义两个卷积层和两个池化层

~~~python
# 模型定义
class CNN(Module):
    # 定义模型属性
    def __init__(self, n_channels):
        super(CNN, self).__init__()
        # 输入到卷积层 1
        self.hidden1 = Conv2d(n_channels, 32, (3,3))
        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')
        self.act1 = ReLU() 
        # 池化层 1
        self.pool1 = MaxPool2d((2,2), stride=(2,2))
~~~


**I. 定义这个方法，后面直接使用即可**

`class CNN(Module):`定义一个名为CNN的类，继承自torch.nn.Module，是PyTorch中所有神经网络模块的[基类（父类）](../面向对象程序设计/基类（父类）.md)

`def __init__(self, n_channels):`CNN类的构造函数，包含2个参数，`self`代表地址，一般不会修改，`n_channels`代表输入图像的[通道](../计算机视觉/通道.md)数

`super(CNN, self).__init__()`调用父类Module的初始化方法，确保正确的初始化模型

**II. 卷积层1**
`self.hidden1 = Conv2d(n_channels, 32, (3,3))`第一个卷积层接收`n_channels`个输入通道，并输出32个特征图（输出通道数为32，即这里有32个卷积层用来处理训练数据集，每个[卷积核](卷积核.md)可以提取一种特征，最终得到32个映射），使用的是3×3的[卷积核](卷积核.md)进行卷积运算；`Conv2d`是PyTorch中用于创建二维卷积层的类

`kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')`使用`kaiming_uniform_`初始化权重并设置[激活函数](激活函数.md)为ReLu，深度学习模型训练过程的本质是对weight（即参数）进行更新，这需要每个参数有对应的初始值，`self.hidden1.weight`表示卷积层1的[Tensor张量](Tensor张量.md)

`self.act1 = ReLU() `创建一个ReLU[激活函数](激活函数.md)并将其赋值给act1属性

**III. 池化层1**

`self.pool1 = MaxPool2d((2,2), stride=(2,2))`将输入的特征图分割成(2，2)大小的区域，并从每个区域中去最大值，`stride=(2,2)`是步长，决定了[池化核](池化核.md)在输入特征图上的滑动速度。

通过池化，在保留最重要特征的同时，图的尺寸再一次缩小，减少了模型的计算量和参数数量，同时增加在不同模型之间的通用性（减少过拟合，提高泛化能力）

![](附件/池化层.png)


# 建立多层数字识别CNN神经网络模型

在单层神经网络的基础上添加新的卷积层和池化层

~~~python
# 模型定义
class CNN(Module):
    # 定义模型属性
    def __init__(self, n_channels):
        super(CNN, self).__init__()
        # 输入到卷积层 1
        self.hidden1 = Conv2d(n_channels, 32, (3,3))
        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')
        self.act1 = ReLU()
        # 池化层 1
        self.pool1 = MaxPool2d((2,2), stride=(2,2))
        # 卷积层 2
        self.hidden2 = Conv2d(32, 32, (3,3))
        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')
        self.act2 = ReLU()
        # 池化层 2
        self.pool2 = MaxPool2d((2,2), stride=(2,2))
        # 全连接层hidden3
        self.hidden3 = Linear(5*5*32, 100)
        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')
        self.act3 = ReLU()
        # 全连接层hidden4，同时也是输出层
        self.hidden4 = Linear(100, 10)
        xavier_uniform_(self.hidden4.weight)
        self.act4 = Softmax(dim=1)
~~~


**I. 卷积层2**

`卷积层 2`第二层卷积进一步提取特征从第一层卷积和池化后得到的特征图中的特征

`self.hidden2 = Conv2d(32, 32, (3,3))`中接收的通道数修改为32，即上一层中输出的32个输出通道

**II. 池化层2**

`池化层2`通过减少特征图的空间尺寸来降低后续网络层的计算负载并在一定程度上增强模型的抗噪声能力

`self.pool2 = MaxPool2d((2,2), stride=(2,2))`和之前一样使用最大池化策略

**III. 第一个全连接层hidden3**

`全连接层`对前面卷积层提取的特征进行整合以进行最终的分类决策，经过2次的卷积和池化图像的特征已被有效提取并压缩，全连接层的作用类似于神经网络中的“连接器”，能够将前一层的所有神经元与当前层的每个神经元相连接

`self.hidden3 = Linear(5*5*32, 100)`接受了来自第二个池化层的800个视觉信号，这些信号已经被整理成了一维的线索，处理中心的目的就是将这些线索转化为100个有意义的概念

`kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')`使用`kaiming_uniform`来初始化处理中心的连接权重

`self.act3 = ReLU()`引入`ReLU激活函数`给对全连接层的输出进行非线性变换

**IV. 第二个全连接层hidden4 / 输出层**

`self.hidden4 = Linear(100, 10)`将之前的100个概念简化为10个最终的决策，每个决策都对应着一个手写数字的类别（0至9）

`xavier_uniform_(self.hidden4.weight)`使用[`xavier_uniform`](xavier_uniform.md)方法来初始化权重，为神经网络的每个连接都赋予了合适的权重

`self.act4 = Softmax(dim=1)`引入`Softmax激活函数`将决策中心的输出转换为概率分布，能够根据概率最高的类别来做出最终的预测


# 前向传播(Forward Propagation)

## 1. 前向传播的原理

前向传播是数据在神经网络中的流动方向即从输入层经过隐藏层最终到达输出层的过程

前向传播是神经网络处理输入数据并生成预测结果的过程，是神经网络的基础也是整个机器学习中的核心概念之一

### 1.1 前向传播的作用

1. 计算输出:根据输入数据和网络结构计算神经网络输出
2. 损失函数评估:使用输出数据与真实标签计算损失函数以评估模型的性能
3. 反向传播准备:为反向传播阶段提供必要的中间信息，如每层的输入、权重和激活函数的导数等

### 1.2 CNN架构和前向传播之间的关系

* 构建CNN架构：定义了神经网络的各个层及其参数，卷积层、激活函数、池化层、全连接层和输出层。定义了前向传播的路径和操作顺序。


* 前向传播：数据在已构建的CNN架构中流动和处理的过程依次经过每一层的处理，逐步提取特征，生成最终的输出结果。定义了路径上执行具体的数据处理层与层之间的数据传递和参数计算。


## 2. 反向传播

在前向传播之后进行的，神经网络会通过反向传播算法来调整权重和偏置，以减少损失函数的值，目的是计算损失函数关于网络参数的梯度，以便通过梯度下降等优化算法更新网络的权重

## 3. 前向传播的代码实现

~~~python
# 前向传播
def forward(self, X):
    # 输入到隐层 1
    X = self.hidden1(X)
    X = self.act1(X)
    X = self.pool1(X)
    # 隐层 2
    X = self.hidden2(X)
    X = self.act2(X)
    X = self.pool2(X)
    # 扁平化
    X = X.view(-1, 4*4*50)
    # 隐层 3
    X = self.hidden3(X)
    X = self.act3(X)
    # 输出层
    X = self.hidden4(X)
    X = self.act4(X)
    return X
~~~

`def forward(self, X)`这个函数中每个语句的输入输出都是训练集X，每个语句都控制训练集在之前定义的模型层中进行训练，`self`和后续语句的`self`都表示我们在上一步中定义的模型实例

这些神经网络层接受我们的训练集（自变量）并返回经过训练的自变量，以此类推，这便是在pyTorch中定义前向传播的过程。

在前向传播过程中，每个步骤都是一个神经网络层，数据流转方式相应的由我们定义的方法所确定

### 3.1 数据进入第一卷积层并处理

`X = self.hidden1(X)`将输入数据X输入卷积层1，卷积层会对图像进行过滤操作

`X = self.act1(X)`激活函数，使用`ReLU激活函数`，使用的激活函数类型则在CNN架构中被定义，将卷积层的输出进行非线性变换

`X = self.pool1(X)`池化操作

### 3.2 数据进入第二卷积层并处理

### 3.3 数据扁平化

`X = X.view(-1, 4*4*50)`[Flattening数据扁平化](Flattening数据扁平化.md)具体来说，-1 表示自动计算的批次大小，5×5×32 表示每个样本的数据维度。

### 3.4 数据通过全连接层

### 3.5 数据通过输出层

`Softmax激活函数`的并且所有输出值的总和为1

### 3.6 返回结果

`return X`

前向传播的最终结果是经过所有层处理后的输出，这些输出将用于模型的预测或损失计算

# 计算误差

引入[损失函数](损失函数.md)，用于衡量模型预测的准确性，帮助我们了解模型当前表现，指导我们如何改进

# 反向传播和参数更新

通过反向传播算法，将误差从输出层逐层传递回输入层并根据这些误差调整每一层的参数

[反向传播](反向传播.md)：负责计算损失函数相对于每个参数的梯度，这是优化器所需的关键信息

[优化器](优化器.md)：用于调整模型的参数以最小化损失函数，提高预测性能

# 训练模型

~~~python
# 训练模型
def train_model(train_dl, model):
    # 定义优化器
    criterion = CrossEntropyLoss()
    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)
    # 枚举 epochs
    for epoch in range(10):
        # 枚举 mini batches
        for i, (inputs, targets) in enumerate(train_dl):
            # 梯度清除
            optimizer.zero_grad()
            # 计算模型输出
            yhat = model(inputs)
            # 计算损失
            loss = criterion(yhat, targets)
            # 贡献度分配
            loss.backward()
            # 升级模型权重
            optimizer.step()
~~~

## 1. 定义损失函数和优化器

` criterion = CrossEntropyLoss()`定义了一个交叉熵损失函数（CrossEntropyLoss），是分类问题中常用的损失函数

`optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)`定义了一个随机梯度下降（SGD）优化器，`model.parameters()`表示优化器将更新模型的所有参数，`lr=0.01`学习率（learningrate）决定了每次参数更新的步长大小，`momentum=0.9`动量(momentum)通过在更新方向上加速，帮助优化器在鞍点附近更快地收敛并减小震荡，目的是为了加速收敛并避免陷入局部最优解

`for epoch in range(10):`循环遍历10个训练周期（epoch），一个epoch指的是一个数据集完整地进行一次前向和后向传播的过程，多次遍历可以使模型更好地学习数据的特征，提高模型的泛化能力。通过多次epoch后[TrainLoss训练损失](TrainLoss训练损失.md)降低，[TrainACC训练准确度](TrainACC训练准确度.md)和[TestACC测试准确率](TestACC测试准确率.md)提升。

`for i, (inputs, targets) in enumerate(train_dl):`遍历训练数据加载器中的每一个mini batch，mini batch是一小部分训练样本的集合，其有利于模型稳定训练提高计算效率