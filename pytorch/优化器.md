根据损失函数所提供的反馈来调整模型的参数，决定了如何以最有效的方式更新参数以最小化损失函数

不同的优化器有不同的更新规则和特点

* 梯度下降：最基本，但可能收敛慢或者陷入局部最优
* Adam优化器：通过自适应学习率调整，常能更快地收敛到全局最优或者较优的解